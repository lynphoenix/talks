# 提纲
* 分享的目的是什么
* 漫谈人工智能
  * 为什么要研究人工智能
    * 什么是智能
    * 什么是人工智能
    * 人工智能的层次
    * 人工智能的方向和应用领域
    * 人工智能的发展

  * 为什么是深度神经网络
    * 时代背景
      * 数据爆炸
      * 硬件升级
    * 视觉算法
* 谈一点计算机视觉算法
  * 分类算法 - CNN
  * 检测算法
  * 分割算法
  * 跟踪算法
  * 语义描述

* Ataraxia干什么
  * SWOT的分析和思考
  * 战略方向
  * 算法建设的重点
    * 分类问题工程化
    * 检测问题精细化
    * 分割语义建设闭环
    * 数据团队扩大化
  * 通用检测算法重点
    * 工程化
    * 分类词条梳理
    * 场景切割 - 检测 - 跟踪 API框架建立
    * 无监督强化
      * 半监督打标
      * 跟踪增加样本
      * 分类增益检测
      * GAN样本放大


## 开题 - 分享的目的
大家下午好。
今天我分享的主题是“漫谈人工智能”，也就是随便聊聊的意思。人工智能这个命题这两年越来越火，还有更火的趋势。其实我们部门早就应该来跟大家漫谈一下的。但是我也有些惶恐，生怕大家早就听多了看多了，已经审美疲劳了。我本来是的分享是面向团队内部做算法方面的分享，因为我希望我们的工程团队能更好地了解算法的一些基本概念，了解一下算法团队的工作内容，也给算法团队更好的支持。又因为之前在群里聊起过，所以就多问了一嘴，那如果有其他同事感兴趣的话就一并来聊聊，因为我也喜欢把这些信息传递给更多的人。结果一问就出事了，发现还是有很多人并没有审美疲劳的。既然大家这么有热情，那么我也多扯一扯。我就以一个从传统机器学习过度到新时代的深度学习的研究人员的角度来谈谈这个话题。

首先来谈第一个问题，我们为什么要研究人工智能。
## 为什么是人工智能
我觉得我们人类的存在是一个莫名其妙的事情。我们与这个自然环境是那么格格不入，你想象一下有人类存在之前的世界，再看一眼今天的世界。我总觉得人类的出现像是生物界进化的一个意外，一个bug；而不是进化的终点。所以我们常常会觉得很孤独上，从诞生第一天起，我们就渴望去了解这个世界。
我们探索世界的方向有很多，我们仰望星空，企图知道遥远的星球上是不是有和我们一样的存在，这个世界的边界在哪里，边界的外面又是什么。
我们想了解时间的边界在哪里，宇宙大爆炸之前是什么，这个世界有没有终结的一天？或者时间的存在形式跟我们以为的，我们感觉到的是不是一样。
我们也想知道是什么组成了我们今天的世界，各种物质，它们的属性是怎样的，生命是怎样诞生的，人类是怎样诞生的。在这一切当中，也许我们首先最想要了解的是我们自己：我们每个器官是如何构成我们这样的一个系统，怎么运行，为什么会有生老病死，有没有办法达到不朽。我们的大脑又是怎样的？我们的意识是怎么来的？智能又是个什么东西？
人工智能就是尝试去了解我们的大脑，更确切的说，是要去了解智能是什么的一门学科。而人工智能的研究路径可以可以总结为Richard Feynman的这句话：What I cannot create, I do not understand. 我们通过创造一个事物去了解它。
所以人工智能的研究，就是希望通过设计一个真正能在人类所处环境中表现出像人类一样智能行为的计算机，来理解我们的大脑，理解智能。

### 什么是智能
那么第二个问题，就谈谈什么是智能？人类如何通过智能，一步步构建成了今天的社会风貌。
传统的观点是智能来源于语言。但是实际上我们人类的语言不是世界上第一种语言。我最近看了一本书，叫“人类简史”。这本书提到，青猴就有不同的语言来表达不同的意思。比如“老鹰来了”和“狮子来了”就是两种表达接近而含义不同的声音。研究人员把“老鹰来了”放在一群青猴听，猴子们立刻停下各种动作，恐惧地望着天空。而播放“狮子来了”，猴子就纷纷往树上爬。看到了吧，猴子对语言有明确的反应。

因此，如果说人类的智能在于语言的话，那么人的语言有什么优越性呢？还是这本书提到了，使得人类语言与众不同的，不是描述“老鹰来了”，“狮子来了”这类周围世界信息的能力，而是描述人类社会信息的能力。这是什么能力呢？--八卦！八卦的能力对于人类进行社会协作至关重要，可以说是在残酷世界里人类部落生存和繁衍的关键。为什么呢？因为八卦的能力能够告诉我们这个部落里谁和谁关系亲密，谁讨厌谁，哪个人靠谱，谁的老婆或老公不能睡。据说在7万年前，我们的祖先能够靠这种技能八卦数个小时之久。千万不能小看八卦能力，这个能力是使得人类部落的规模能够扩大的关键因素。据说我们的祖先智人在10万年前走出东非大草原跟尼安德特人打了一架，然后被干回了东非老家。然而再过了3万年，由于掌握了“八卦”这一核心科技，再次出征亚欧大陆，就把其它邻居表亲都干没了。为什么呢？因为八卦使得智人的协作能力和部落规模扩大了数倍。
扩大到什么程度呢？社会学家的研究表明靠八卦能力能够维持的关系规模大约是150人。这点到了今天也基本没有改变。比如七牛在100多号人的时候基本靠大家自觉就组织的很好了，而到了500多人的时候，老许就开始研究管理了。

那么这150个人的部落是否能组成今天这个社会的风貌呢？当然是不行的。如果停留在这个层次，我们可能只是较大的部落而已。想要像今天这样形成一个个的国家，民族，还需要智能或者语言的第三个层级的飞跃，就是传达关于虚构概念的信息。也许这才是人与其它物种的最大的区别。人类会相信一些虚构的概念，事实上构成我们今天社会稳定、推动世界发展的都是我们共同定义的一些虚构的概念。比如宗教，国家，公司，金钱，善恶，道德。我们对这些非物质的概念深信不疑，使得人类能够突破150人的圈子，建立起大量陌生人之间的合作关系。国家的兴起，大规模战争的兴起，公司的兴起都是因为我们能相互之间传达和理解关于虚拟概念的信息。

以上的三个层次是从社会学的角度来看的智能带来了什么，我们的社会是怎么构建出来的。然后再看下一个问题：什么是人工智能。


### 什么是人工智能
我们一直说人工智能是一个发展中的学科。那其实有一句潜台词其实就是，我们没有非常精确的对人工智能的描述。所以我在这里，仅是提供一些大人物们的思考角度。

* 图灵测试

首先看被称为“人工智能之父”的图灵怎么看这个问题。他获得这个称号的很大的原因在于写了一篇论文《计算机器与智能》。在开篇的“Can Machine Think?”里，他提出了著名的图灵测试：如果在一个回答游戏中，机器能够骗过问问题的人，使得对方觉得回答问题的是人而不是机器，那么就认定这个机器是有智能的。
在图灵看来，人类的智能最伟大的能力在于对抽象符号的处理 -- 恰好计算机也能做到。所谓听和说的过程，就是我们用恰当的语法规则对被称为“词”的概念符号的处理的过程。那看东西呢？我们是用概念符号代表物体以及物体的位置、名称和其它属性(这点在后面谈视觉的时候也会提到类似的意思，相当于高层次的视觉特征)。再比如AlphaGo下棋呢？处理的是代表不同旗子的属性和位置的概念符号。
而将智能的能力简化为抽象符号的处理(也就是计算)之后，就可以套入图灵机的理论了：人工智能，无非也就是一台图灵完备的机器，对吧。

* 神经元的数学模型

神经科学的发展似乎为图灵的假说提供了支持。1943年，Warren McCulloch和Walter Pitts提出了神经元的数学模型。该模型认为，大脑的神经元的工作方式跟数字电路中的逻辑门是一样的。如下面图中看到的，就是一个多输入线性加权，然后通过一个激活函数输出0或1。这个基本单元跟逻辑门是一样的。这个模型现在已经被广泛接受，至少好像没有人针对这个问题提出过有说服力的质疑。
而我们知道，现在计算机的所有芯片都是这样的逻辑门构成的，不论是CPU还是内存。所以在研究人员来看，人脑和计算机的基本工作原理是一致的。这也使得有很多人觉得没有必要研究大脑，而应该把精力放在设计更好的算法和理论的研究证明上。人脑有太多软弱自私等没有意义的主观情感，研究这么一个混沌的系统，对人工智能毫无意义。计算机可以做到人脑所能做到的一切，甚至更多，完全不需要研究大脑这一生理混乱无法理解的结构。

* 中文屋实验

“中文屋”实验室对这一流派的一次重要打脸。这个实验是UC Berkley的著名哲学教授John Searle专门设计来打脸人工智能的。
这个实验是这样的：让一个只懂英语的人坐在一个房间里，手头有纸笔还有一本指令手册，介绍如何处理汉字。这里的处理只涉及复制、删除、插入、重排等语法指令，而并不描述任何汉字的含义。
外面的人向里面递进中文写成的问题，里头的人按手册的说明，将中文符号组合到一起写到纸上，并最后按指令将纸条递出去。
最后别人再问外面的人：这个人会不会中文。外面的人说“必须的！”
而实际上，屋里的人(相当于CPU)，不懂中文；指令手册(软件)，只是一堆正则而已，也不懂中文。实际上，这个指令手册可以认为是人试图通过编程让计算机模仿人类的行为而积累智能的结果。John Searle的意思的，无论怎么设计这个程序，计算机都不可能具有理解力，也就不可能具有智能。他说“我也不知道智能是什么，但不论智能是什么，计算机都不具备”。

以上是关于什么是人工智能这个命题的比较重要的几个思考。可以看到，观点完全无法统一。发展中的学科就是这样。就跟什么南海问题钓鱼岛问题一样。吵吵吵到最后，结论只能是“搁置争议，共同开发”。或许有一天，我们能模拟人脑的一切输入和输出了(或者如我最近常想，如果能为大脑植入一个外挂系统，能够实现到今天为止的所有知识的快速检索和理解，有一个计算机与人脑的接口而不是用电脑去思考)，这些层面之间的相互联系可能会一目了然。但是到今天为止，这个问题还有有很大的哲学属性。多思辨有好处，但囿于争论就没有意义。我们从业人员，应该做一些更加实际的工作。

### 人工智能的三个层次
比如，给人工智能问题分分层级：
最底层的层次是“感知”。其实在我看来感和知也可以分为两个小层。所谓“感”，就是将外部输入转换为大脑能够接受的信息；如果用计算机的语言来说，相当于传感器。视觉的传感器就是摄像头，听觉的传感器就是麦克风，触觉的传感器可以是压力感应器，等等。计算机能接受的传感器信号是整数或浮点数，不论是摄像头还是麦克风，最终是要通过模数信号转换形成一串无聊的数字才能被计算机所接受；而大脑也是一样，不论是视觉信号还是听觉触觉，都需要转换成生物电信号进入神经元，才能被感知。而所谓“知”，就已经是很高级的能力了。看到一个脸知道它是脸，看山是山看水是水，这个叫做“知”；听到一串声音，能辨认出它是“梦幻曲”，还是有人在喊你“吃饭”，这个是“知”；摸到一个圆圆的东西，知道它是鸡蛋，这也是“知”。但是你一定已经发现，这个“知”其实已经包含了基本的鉴别、判断、语音到文字的转换等过程。
第二个层次是“决策”。决策是在已知信息的基础上进行更高层次的判断。比如你走在街上，看到有个人向你跑来，你判断他的轨迹并不会跟你撞上；再判断他体型瘦小，速度不快，哪怕要撞上你也有足够的能力自保；此人手捧鲜花，眼神放光，你判断他是沉浸在欢愉的爱情之中，那么断不会对你造成生理上的威胁。在不到一秒钟的时间内，你已经根据“感知”层得到的各种信息，做出了三次以上的判断，并且这个判断，决定了你的行为，是避让？还是迎上？或者忽略这个人。
第三个层次是“反馈”。为什么反馈不是前两个层次中的一部分？因为反馈是构成一个有执行能力的人工智能系统的重要独立环节。它大量应用在机器人系统中。对于机器人系统来说，你可以认为反馈就是执行机构的监控系统。当无人驾驶的汽车执行刹车指令的时候，它需要反馈系统来传达刹车是否成功。另一方面，反馈也是学习的重要手段。我们大脑学习所有知识都是通过“预测 - 反馈”的路径进行的。你走进房间，看到我这个人。你脑子里会先浮现出我这个人的样子，然后再与你看到的验证。眼睛是眼睛，鼻子是鼻子，哦所以这是一个人。如果长眼睛的位置长了两个鼻子，那你一定会惊醒，然后你脑子里一些原本并不活跃的神经元开始变得活跃起来，开始学习“这个人的眼睛上居然长的是鼻子”。我们脑子进行“预测 - 反馈”的频率比你意识到的要多的多,这个过程无时不在进行。实际上我们的大脑是通过感受到的事物与预测的模式之间的匹配来学习的，这个概念可能跟传统的概念是不一样的。这个匹配的过程，靠的就是反馈。靠预测去泛化，用反馈来收敛。这是学习的原理。


### 人工智能的方向和应用领域
聊过了什么是人工智能之后，稍微来看一看这个领域的研究方向和应用。
左边罗列的是最近两年的研究论文和专利申请较多的研究方向。主要是语音，自然语言处理(NLP)，视觉，机器人，浅层机器学习和深度神经网络。
需要指出的是，语音和自然语言处理是两个不同层次的事情。前者是能将声音转换为文字，而后者则是要理解文字的意思。前者现在已经可以说比较成熟了，而后者还表现的像个智障。
另外，浅层的机器学习和深度神经网络指的都是算法方面的演进。而深层和浅层，实际上指的是学习算法的层数，而不是算法本身的能力深浅。其实感觉上这是深度学习业界故意挤兑人，打嘴炮的说法。
右边罗列的是人工智能的应用领域。

* 人工智能的应用领域
  * 个人助理（智能手机上的语音助理、语音输入、家庭管家和陪护机器人） 产品举例：微软小冰、百度度秘、科大讯飞等、Amazon Echo、Google Home等
  * 安防（智能监控、安保机器人） 产品举例：商汤科技、格灵深瞳、神州云海
  * 自驾领域（智能汽车、公共交通、快递用车、工业应用） 产品举例：Google、Uber、特斯拉、亚马逊、奔驰、京东等
  * 医疗健康（医疗健康的监测诊断、智能医疗设备） 产品举例： Enlitic、Intuitive Sirgical、碳云智能、Promontory等
  * 电商零售（仓储物流、智能导购和客服） 产品举例：阿里、京东、亚马逊
  * 金融（智能投顾、智能客服、安防监控、金融监管） 产品举例：蚂蚁金服、交通银行、大华股份、kensho
  * 教育（智能评测、个性化辅导、儿童陪伴） 产品举例：学吧课堂、科大讯飞、云知声

要我说这些领域的经济价值都很大，有很多可以做而还没做好的机会。至于这些机会是否属于人工智能，我看未必。总的来说，自动驾驶、医疗、安防、金融、电商这些领域，我觉得有比较清晰的未来，能很清晰地看到人工智能的进步带来的质变。而个人助理、教育，我认为目前的机会还是在于资源和大数据的整合，本质上还是由产品经理决定的；从我的粗浅的眼光，还看不到目前的算法能力能够给这些领域带来很大的机会。如果哪天NLP进一步突破了，或许情况会有不同吧。但这需要很多顶尖算法团队的努力，而不是各小算法应用团队的努力。

### 人工智能的发展

* 达特茅斯会议

要谈人工智能的历史，怎么都得提到1956年的达特茅斯会议，这个会议被公认为是人工智能的研究起点。会议的召集者麦卡锡给这个活动起了个名字叫“人工智能夏季研讨会(Summer Research Project on Artificial Intelligence)”。这是“Artificial Intelligence”这个词第一次出现在大众的视野中。
当时参会的有六位重量级人物：
麦卡锡是会议的召集者，当时是达特茅斯学院数学系的助理教授。后来发明了LISP语言。
明斯基，普林斯顿的数学博士，论辈分的话麦卡锡是他的师叔。他的博士论文做的就是神经网络。他设计了第一台能够自我学习的人工神经网络计算机，建立了MIT的人工智能实验室，也是公认的人工智能奠基人之一。他还发明了共聚焦显微镜，发明了头戴式显示器(今天的VR的鼻祖)。他有个比他小一岁但是比他早4年博士毕业的师兄更加出名，就是“美丽心灵”电影里的主人公纳什，就是因为“博弈论”获得诺贝尔经济学奖的纳什。
塞弗里奇是模式识别的奠基人，写了第一个可工作的AI程序。他是维纳(就是我们上学时候学的那个维纳滤波的主角)最喜欢的学生，他跟提出神经元数学模型的Warren McCulloch是同事。
克劳德 香农，信息论的创始人。香农这样的大神应该不用多加介绍了，只用说一句，他和爱因斯坦、图灵是一个级别的人物，大学时候的“信号与系统”就是拜他所赐开的课。他在二战时一直研究密码学，参会的时候已经是贝尔实验室的大佬了，相当于今天我们各路会议花大价钱请的重磅嘉(花)宾(瓶)。当时麦卡锡和明斯基都曾经在贝尔实验室当香农的小弟。
纽厄尔和西蒙(Herbert Simon)：他们创立了卡内基梅隆大学(CMU)的人工智能实验室。搞计算机和控制的人都知道这是何等江湖地位。他们俩开创了人工智能的符号流派。所谓符号主义是人工智能的一个重要流派，它的核心思想是用逻辑推理的法则，从公式定义出发推演整个理论系统。他们希望人工智能能像经典力学领域的牛顿三定律一样优雅，用三个公式解决经典力学的一切问题；能像电磁场领域的麦克斯韦方程组一样优雅，用四个公式解释电磁场领域的一切问题。但是这个世界的发展还是跟他们的想象不一样，今天我们热衷神经网络，这个流派已经没有太多声音了。
顺便提一句，人工智能的另外两个流派直到今天仍然非常重要：分别是行为主义和连接主义。
行为主义起源于控制论，鼻祖就是上面提到的维纳滤波的那个维纳。控制论以前主要研究的是导弹的飞行轨迹控制、化学过程控制这类复杂的多变量复杂系统的优化问题。当它把魔爪伸入人工智能领域时，主要研究的是如何模拟人和控制过程中的智能行为。人工智能领域中的机器人控制领域就是它的大本营。也就是人工智能三个层次中的“执行”部分。
也幸好行为主义的主要战场是“执行”层，所以在今天神经网络大行其道的时候还能保留一小块阵地而未沦陷。不像符号主义基本上已经溃不成军了。没错，最后一个也就是今天最重要的一个流派，就是连接主义。各位看官一定已经猜出来了，连接主义流派的领衔主演就是今天的当红炸子鸡 - 深度神经网络。

当当当，主角正式登场。

有位著名的爱唱歌的老板的歌是怎么唱的来着，“怎么大风越狠，我心越荡。。。”

你看深度神经网络今天风光无限，但是这半个多世纪可不是这样走来的。看过了世事沉浮，或许你会对深度神经网络的未来有那么一丝担忧。为什么古人要“以史为镜”，因为历史能给你提供辽阔的视角：眼看他起高楼，眼看他宴宾客，眼看他楼塌了。

简单地说，开始是上面提到的Warren McCulloch & Walter Pitts提出了神经元的数学模型。
然后一个叫Rosenblatt的年轻科学家提出了一个叫“感知器”的两层神经网络，并演示了一下这个网络能学习识别简单的图像。然后20世纪50年代的人民像今天的人民一样好骗，大家纷纷觉得神经网络好强大(AlphaGo好强大)，智能的时代来到了，人类马上要被机器取代了！于是企业界和军方都大大资助了人工智能项目，第一次高潮来到了！想想当时正是冷战开始的时候，军方对科技的狂热是我们今天难以想象的。

但是实际上从我们今天的角度看，这个感知器只能做简单的线性分类任务。于是大量人力财力投入了几年之后仍然没什么进展。终于上面提到的另一位大佬明斯基看不下去了，站出来说“你们别闹了，别不把钱当钱了”。1969年，他写了一本书叫《Preceptron》，他用数学证明了感知器连简单的异或问题都解决不了。大家无力反驳，于是纷纷转向研究其他。这是第一次人工智能也是神经网络的第一次寒潮。

到了1986年，深度学习的鼻祖Hinton出马了。他提出了两层神经网络就能解决异或问题，而且提出了反向传播算法来解决了网络的计算。然后大家惊喜地发现，一层的感知器不够牛逼但是二层的神经网络很牛逼啊，可以逼近各种函数，还可以做图像识别自动驾驶。Magnificent！于是大家又开心地进入了这一领域。各种金钱、人才都投入了这一领域。

但是很快大家发现了新的问题，网络一大，很难训练很难优化啊...直到鄙人上大学那会，介绍神经网络的定语还是“调参困难”。
恰好这时，另一位大神提出了一种叫做支持向量机(SVM)的算法，于是机器学习类的算法迅速成为主流。神经网络再次进入低谷。但是这次，人工智能仍然以自己的节奏前行着。

历史的车轮滚滚向前，到了2006年，深度学习的鼻祖Hinton憋了20年，憋出了一个大招 - 深度置信网络(别管置信不置信，就是神经网络)。他提出了两个方法，降低了神经网络的训练难度，一个是预训练加微调，一个是逐层优化。有了这两样利器，网络层数可以大幅增加，两层增加到两百层，不在话下！Hinton还玩了个小心思，他怕“神经网络”被拒稿，把这种算法的名字叫做深度学习(大神当年也是小媳妇~~~)。

虽然这篇文章发在了Science上，但是仍然没有引起大家的重视。于是他老人家生气了，带着学生搞了个CNN，亲力亲为参加ImageNet竞赛。2012年的时候(为什么是12年，因为06年还没有ImageNet，哈哈哈)，他们参赛的结果直接超越第二名10个百分点以上(74% vs 85%)。这下大家不淡定了。纷纷开始研究为啥你的算法这么牛逼啊。加上这个时候开源的思想伸入人心，老人家也赶新潮，把自己的程序开源了。结果人家说“你的程序里有bug”。一看真的有bug，问题是有即使bug，效果还比人家好10个点。这让人如何是好...

再后来的故事大家都知道了，仿佛是yesterday once more。大家纷纷觉得智能的时代来到了，人类马上要被机器取代了，AI科学家的年薪赶上C罗梅西了...

以上纯属八卦。为什么要聊这么久的八卦呢？想必大家也发现学术圈的八卦其实也挺多(这还是笔者做了滤波的结果，放开了聊三天三夜不在话下)。我们看学术圈的八卦就像小时候看希腊神话的感觉一样。小时候看希腊神话感觉这些神跟中国神话中的神仙们差别而太大了，中国神话中的神仙都是脸谱化的好坏分别。而希腊神话里的神，那哪叫神啊？！有偷看姑娘洗澡的，有搞大人家肚子的，有偷汉子的，还有无照驾驶开车失控撞死自己的...简直没有一个正常的。长大点了我开始觉得这叫人本主义...

我们看学术圈大神们的感觉也是一样，抛开头脑发达与否不谈，他们的生活也是鸡零狗碎，跟那些A睡了B的老婆，C嫁了D再离婚改嫁E最后跟F上床等摇滚乐队故事没什么区别。而且他们的青春时代正是摇滚乐，抽大麻，反越战，要做爱不要作战的时代。所以啊，有时决定你能走多远的，不是你生活的苟且与琐碎，而是你是否志存高远，能不能持续去追求你的兴趣所在。也许你对今天自己做的事情并不满意，但如果你以宏大的时间跨度去看，只要志存高远，思路清晰，脚踏实地，或许你就成为了某个领域的奠基人！

## 二 为什么是深度神经网络
这个问题其实本质上跟冷战时期大家对人工智能的渴望是一回事，今天深度神经网络的成功，也是因为这个时代的时代背景。

那么时代背景是什么呢？就是数据爆炸和硬件升级。
通讯行业和互联网经过这么多年的发展，现在已经进入数据量极大的时代。这点已无需多言。一方面是网络越来越发达，一方面是云计算的落地导致越来越多的数据堆积在云端，还有就是通讯带宽和个人终端的升级导致数据的生产量也越来越大。在做研究方面我感受最深刻的是，在学校期间做计算机视觉的样本量级还是几千到几万，能找到几万图片的数据集都两眼放光。找到几十万的数据集，放光也没有，下载都下载不来，下来了也没地方放。而现在随随便便一个项目没有上百万的数据量都不好意思打招呼。
数据量级的改变带来最直接的结果是，原来打的神经网络没有还手之力的SVM和AdaBoost等工具不灵了！收敛不了了！怎么办？产业升级啊！求适应海量数据的新的算法工具呀！

我们把问题Zoom In一下。传统机器学习为什么会有这个问题呢？
笔者还在校那会做机器学习的典型流程是这样的：
样本 -> 设计特征 -> 判别算法筛选组合特征 -> 结果
本质上，特征是一种数据的映射。如果往高维数据上映射，那是为了稀疏化；如果往低维数据上映射，那是为了简化问题。然后判别算法从提取特征中提取特征或者组合特征，来得到最后的算法模型。这个过程有什么问题呢？问题出在“设计”上。设计的意思就是自己想出一个特征，俗称拍脑袋。拍脑袋是什么意思？意思是没有上帝视角，没有高屋建瓴的理论指导，说的好听点叫工程直觉，说的不好听点就是靠运气。科学史又称拍脑袋史。牛顿被苹果拍了下脑袋，拍出了万有引力。门捷列夫梦到了蛇，设计出了元素周期表。如果你设计了视觉算法中的SIFT或者HOG这样成功的特征，那么恭喜你进入了名人堂。但要说这个SIFT背后是什么原理？我会告诉你在其基础上诞生的所有理论都只是为了解释而解释。直白点的，像最简单的Haar特征一样，科学家看看猫的眼睛(因为他们没法解剖活人的眼睛啊，也有可能解剖过了，但是不敢说...)觉得是这么工作的，于是计算机视觉的研究者搬过来用，得到一个很通用的特征。
那这些简单的特征如何能够得到判别模型呢？靠的是判别算法的学习。判别算法学的是特征的排列组合。研究学者通过一堆的数学公式证明告诉你，这些特征以某种方式去组合，能够得到一个稳健的算法模型。

那我们再看深度神经网络的典型流程，就知道为什么深度学习得以成功了。
样本 -> 神经网络 -> 结果
这区别就在于设计特征，判别算法的筛选组合被神经网络给取代了。同样是拍脑袋，那么我们自己不拍了，改让神经网络拍去。而且你会发现，之前设计特征和判别算法优化是两个相互独立的过程。特征提取不考虑算法的特性，算法迭代的时候也不会对特征提取过程给出指导性意见。而神经网络的工作方式则完全不同，特征提取蕴含在网络的每层参数之中，它拍脑袋的过程是由学习的目标函数直接指导的。或者说它的特征就不是拍脑袋拍出来的，而是在模型训练的过程中学出来的。这个优点，从神经网络诞生起它就存在。之前的问题是神经网络不好收敛不好调参，但现在Hinton又恰好解决了这个问题。所以说到底，在数据爆炸的时代背景下，有了有效的训练方法后神经网络提取特征和训练模型的能力比我们自己手动设计特征要好是它成功的关键。它集中解决了两个问题，收敛困难和海量数据下的特征设计和筛选。今天你要是拿个10000个样本的分类问题问我，我仍然会推荐你用SVM或者Boost捣鼓一下。但你动不动就要求海量数据下的普适性，我真的很难设计出靠谱的特征，于是我就会推荐用神经网络。

再谈谈第二点，硬件升级。除了说电脑的速度越来越快了，还有一个重要玩家就是不得不提的nVidia。遥想08年的时候，笔者看到nVidia发布了一个叫CUDA的并行程序套件，能在C语言里用GPU来做大规模并行计算。于是在实验室跟博后师兄说“这玩意能改变世界”，还打印了厚厚的一本白皮书来看...可惜啊，笔者编程能力太差，而且当年nVidia对Linux的支持实在太差(其实现在还是差，只不过我们现在用的都是服务器版本的GPU，这个问题被避免了)，导致没能在这条道路上坚持下去。
但是GPU与神经网络确实是天作之合。神经网络的每层都有大量的并行计算。特别是在计算机视觉领域，图像的本质就是并行的每个像素点组成的，nVidia也正是将图像渲染中已经广泛应用的并行技术搬到了高性能计算领域。
实际上当年看到并行计算潜力的不止是nVidia，肥肉大家都看得到。同期还有个OpenCL联盟，干的事情也类似。但是ATI明显没有nVidia的决心大。nVidia花了好多年时间进行了一场豪赌，它把GPU送给各大研究所，手把手教他们用GPU来开发并行计算的程序。他们确实赌对了，深度学习在视觉和语音领域的成功，Caffe这样成功的算法库的腾空出世，让nVidia一下子站到了聚光灯下，把ATI甩的好远。估计黄仁勋的心里在某一个时刻曾感谢过ATI：“如果不是贵司把我的桌面显卡的市场份额打压的那么厉害，我应该不会在cuda上押这么大的赌注，也许就不会有我的今天。”(这个故事告诉我们，你以为你尽力了，和你真的尽力了是差很远的。这世间最怕的就是人家比你出身好，比你聪明，还比你勤奋，然后你天天在做梦超越别人...)
GPU到底给计算带来了多大的加速呢？举个例子，nVidia最新发布的P40官方号称速度是CPU的45倍(因为是多核并行计算，不是单核的处理能力)。我们现在训练的几百万样本级别的分类网络一般的训练周期是10天到一两个月。如果用CPU，那么怎么也得一年吧...
不管什么算法，如果告诉你训练一个模型需要一年，在商业上应该不太会有人为它买单...
所以本质上，深度学习今天在技术上的成功，是数据量、硬件、算法三方面合力的结果。对了，还要感谢开源算法社区和开源论文库ArXiv，大家都懂我就不提了。我也没说商业上和传播上的成功，这方面的话我觉得还要夸一夸Google和AlphaGo。要不是Google有角度而无节操的宣传，这个领域也还是一个小众的春天。体力有限，来不及感谢。


## 计算机视觉算法介绍
以上扯了半天的人工智能，那么落地到我们Ataraxia的内容，主要是计算机视觉。这其中最重要的原因当然是我们七牛云存储的起家就是靠富媒体数据的存储。我们有超过2000亿的图片和超过10亿小时的视频。所以这个是我们从视觉起家的本钱。
当然，另一方面，视觉是深度神经网络最成功的应用。前面也说了，Hinton在ImageNet的图像识别上的成功，引起了学界极大的关注。
而我本人作为一直在视觉方向的研究者，今天也尝试介绍一些跟我们的业务相关的主流计算机视觉算法的原理。
计算机视觉，作为人工智能中感知层的一个断面，它研究的是怎么模拟人眼的功能。
人眼大致有这么几个功能，一是物体的识别，二是确定物体的形状和方位，三是判断物体的运动状态。对应到计算机视觉领域，分别是目标检测和识别；地图重建，运动分析。今天的深度学习，主要是在第一个方面努力替代传统的机器学习方法。恰好我们七牛需要的视觉算法，或者说我们的客户需要的视觉算法的业务，也都集中在视觉识别的领域。后两个领域的应用场合主要是机器人。而一旦牵涉到动作执行，往往对可靠性和准确性以及性能的要求要远大于简单的辅助判断。而神经网络由于可解释性差的弱点，因此在这两个方面的进展仍然缓慢。

具体点说，科学家发现，人类具有多个阶梯级联的，层次级的视觉中枢，参见下图的V1到V5。低级区域的输出作为高级区域的输入。低级的皮层识别局部特征，越往高层，抽象程度逐渐提高。
再看右图的深度神经网络，两者是不是天然的相似？
再有一点就是，视觉处理的对象是图像。图像有两个特征是跟神经网络有关的：一是信息含量非常高，冗余程度也非常高。但是对算法来说其实不容易抽象；这就决定了传统的手工设计特征的方法的效率不高，因此视觉对特征提取和判别算法的提升需求都非常迫切。二是前面说过图像都是对各个独立像素的操作，因此并行化程度非常高。这两点决定了深度学习在这一领域不仅有算法效果的突破，还伴随着算法效率的极大提升。

大家知道，我们去年的主要产品是鉴黄。而我们团队的同学都知道，我一直在说，鉴黄是我们的切入点，是我们的入口。但我们作为一个有逼格的团队，作为一个志存高远的团队，不能停留在卖鉴黄这个低端产品上。
我为什么把鉴黄定义为低端产品呢？我还是从视觉识别算法的层次来说这个事情吧。
识别这一领域的视觉算法从简单到复杂，大致分为这几个层次：
* 最简单的是分类算法，给你一张图片，你说其中有人或者没有，色情或者不色情，这叫分类。鉴黄就是这一层次的问题。深度学习算法最先突破的ImageNet竞赛，也是分类问题。
* 比分类算法更进一步的是检测。检测问题不仅回答是不是，有没有，还要回答在哪里。比如人脸检测就是典型的问题。
* 第三个层次是语义描述。通俗地说，就是看图说话或者看视频说话。比如图中的这个例子。
这三个层次一个比一个更复杂。也就是说，一个比一个含金量高。虽然我没法很定量地说，一个检测的API应该比一个分类的API贵多少钱，而且也不是说做检测问题比做分类问题活的更好更有商业价值。但是从趋势上看，大家会越来越关注高层次的应用；而低层次的应用，由于门槛低，会越来越趋向于成为标准品。

当然，这只是个简单维度的层次划分，还有些很重要也很有意思的方向，也是我们关注的重点。比如跟踪和检测常常是打包在一起的；图像的语义分割也是很重要的方向，在视频直播等场景下有很大的商业价值，对无人驾驶来说更是至关重要。分类问题也不只有简单的“是/不是”这一个维度，比如海量商品的检索，细粒度特征的检索都是分类问题中的高精尖方向。另外，像超分辨率、风格画这些有意思的图片处理算法，也是计算机视觉领域常常受到关注的算法。

那么，我尽量简单地介绍一下跟我们业务相关的识别领域的各个算法。大家会发现，视觉的算法真的很简单。


## 分类算法

### LeNet
先看最简单的分类算法。这是LeNet网络，Hinton的学生Yann LeCun在20世纪90年代为美国各金融机构开发的手写字符识别系统用的网络结构。由于它简单，又具备了现在的深度神经网络的大部分要素，而且效果特别好。因此被大家广泛用于解释神经网络。就像图像处理界总是拿Lena那张照片解释算法一样。话说Lena的真实身份是一个模特，这张照片其实是PlayBoy杂志里的彩页，但也不知为何就在计算机视觉领域被广泛应用了，成为这一领域广大宅男和科学家最知名的女神。她的影响力之大以至于有一次top的学术会议还请她作为嘉宾，不知道是不是跟现在的工业展会请苍老师是一回事...

### 卷积层
扯远了，接着说这个LeNet。它由几个要素组成。首先是Convolution，中文名叫卷积。在神经网络的术语中这个操作就称为“卷积层”。可以看到这个网络中有两处Convolution，代表它有两个卷积层。卷积这个名词听着不好懂，我换个名字相信有点数学知识的人都能明白，叫做“加权求和”。没错，卷积就是一个图像块每个点有不同的权重，加权求和之后作为中心点的输出值。卷积层就是用N个这样的加权模板对图像上的所有图像块求，得到一个N个新的图像。

### pooling 层
然后注意到网络里有两处Subsampling，这个叫降采样。学术名叫pooling层。其实就是缩小图像。只是这里有些不同的方法来选择缩小后图像的每个点的像素值。假设长和宽各缩短一半，那么就是在原图的2x2图像块中选择一个点：如果是4个点的像素平均一下，就叫average-pooling，如果是取4个点的最大值，就叫max-pooling。

### CNN
卷积层和pooling层交替的这么个结构，就是卷积神经网络的典型结构。它的英文名叫CNN。是不是好像在哪里听过。但凡听过一星半点的计算机视觉的介绍，比如人脸识别，应该都听到过这个名字。现在只要是跟视觉识别沾边的领域，全都是用的这种流派的算法。区别仅在于细节不同，损失函数不同。为什么？据刽子手(屠杀过很多青蛙、猴子的科学家们)说，我们的眼睛的工作方式就是这样的。就是上面说的，逐层抽象。所以你看，人工智能有时候是很没道理的，大家都是拍脑袋，无非看谁拍出来的效果好而已。到后来大家聪明了，都去找搞生物的去拜师了。

### 全连接层
第二个pooling后面有两个full connection，就是全连接层。顾名思义，把最后一个pooling输出的每个点和本层输出的每个点之间都连接上就叫全连接。原本卷积 + Pooling这个操作得到的是一个二维图片，全连接就是把最后的二维图片拉直，变成了一维的数组。只有一个全连接层还嫌不够神经网络(非线性程度不够)，还搞两个全连接层。最后再用Softmax函数做个分类输出。从前面的CNN结构，到全连接，最后到Softmax的分类输出，这就是从输入的图像(手写字符)到输出的判别结果(A-Z，0-9)的整个过程。

### 优化过程
介绍完网络架构，深度神经网络的决策过程也就清楚了。当然深度神经网络的神奇不是因为这个决策过程而是因为它的学习能力。它如何学习呢？它的学习方法叫做迭代优化，它有一个确定的、非凸的优化目标(损失函数最小化)，一点一点地向正确的方向拱。所谓损失函数，顾名思义就是衡量这个决策造成了多少损失。什么是损失呢？在分类问题里，就是原本是A，预测成了B。这就叫损失函数。计算机视觉的数学定义往往非常简单。
基本上所有机器学习问题采用的都是迭代优化的方式。深度神经网络采用的优化方法就是BP算法，正是20世纪80年代Hinton提出的优化算法。具体的理论计算过程本文就不展开了。但是要提一点，为什么同样是BP算法，20世纪80年代难以优化而现在这么成功。前面提到了Hinton在2006年用了“预训练+微调”以及"逐层优化"的方式降低了神经网络训练的难度。而到了今天大家的常规做法是用通用的分类网络(即ImageNet分类问题训练的网络)训练好的模型，针对自己应用的具体问题去调优。就是“预训练+调优”的过程。而现在的网络已经足够强大，已经不需要“逐层优化”这一方法了。

OK，这就大体讲完了分类网络的结构，推理过程，以及学习的过程。当然实际上里头的门道是很多的，有很多环节都有不同的方法去调优。但是到今天为止，主流的方法都是在这个体系之内。
分类算法是深度神经网络在计算机视觉领域最早也是最成功的应用，也是其它问题的基础。有了分类算法和CNN的概念打底，我们可以接着谈其它的应用领域了。

## 检测算法
检测算法要回答两个问题：what & where。在有深度精神网络之前我们是怎么做检测算法的呢？通常我们是用暴力搜索来解决where，用上面的分类算法来解决what。我们用一个分类器在全幅图像上用滑动窗口去扫描，也就是在全图的每个位置去问一句“是不是你啊？”而由于要问的东西有大有小，所以通常还要将图像进行不同尺度的缩放，也就是缩放出一个从小到大的N幅图像组成的图像金字塔，在整个图像金字塔的每个位置拿着分类器模型去问“是不是你”。最后，再用“非极大值抑制”的方法将可能重合的检测结果合并去重。这个效率是令人发指的，一张图下来，每个位置问一遍，至少10w次，来一套金字塔，就是百万次判断...
基于CNN的图像分类算法大获成功之后，马上就有人将其用到了检测领域。
一开始是Yann LeCun用CNN模型去滑动窗口扫描，这样做准确率是提升了。但是速度实在是没法忍受。
然后一位算法和代码都已入化境的Ross Girshick大神提出了一种算法叫做RCNN - Region CNN，用显著性特征取代了滑动窗口。所谓显著性特征可以理解为图像中特别引人注意的部分，比如色彩特别鲜艳，纹理特别丰富。大神用这种方法在每张图中提取大约2000个可能出现目标物体的区域，然后在每个位置上用CNN模型去问“在不在啊”。可以看到，2000 vs 百万，优势很明显嘛。
当然这种算法不是深度学习的胜利。不过没关系，大神再接再厉。第二年他又提出了Fast RCNN，第三年又提出了Faster RCNN。考虑到Fastest这个词不好随便用，要不然以后没得写了，所以他结束了灌水。于是在Faster RCNN这个方法中将显著性特征这个提取可能目标位置的阶段也用CNN给取代了。从而检测算法的速度得到了极大的提升，基本上可以达到实时了。
再后来，大神玩了个大的，设计了个一步走的检测网络，再也不用分开“提取可能目标位置”和“你谁啊？”两个阶段了。取了个洋气的名字叫做YOLO - you look only once。它的效果并不如RCNN系列，但好处是“特！别！快！”。速度是Faster-RCNN的20倍。所以啊，老同志没有出“Fastest RCNN”，必有深意啊！
有了之前的分类问题打底，我们稍微介绍一下YOLO的网络结构。可以看到它也是卷积 - MaxPooling，卷积 - MaxPooling...然后全连接。最后输出层是一个7x7x30的矩阵。它将整个输入图像分成了7x7的网格，这个7x7x30的矩阵就代表了每个网络位置的预测。30是什么？每个网格会预测各个目标类别的分类概率C(有20个分类因此C=20)，会预测2个目标框，每个框对应x,y,w,h的坐标描述，以及目标框的置信概率s，所以每个框一共是5个参数。20 + 5 * 2 = 30。网络结构介绍完毕。
然后是什么呢？损失函数。检测问题和分类问题的不同，区别就在这个损失函数上。前面说了，分类问题的损失函数就是A判别成B了，损失！前面还说了，检测有两个问题：what和where。所以检测问题的损失函数有两项：是/不是，位置偏差。
对了RCNN系列和YOLO系检测算法在ImageNet的Detection Session上的平均精度能达到60%左右，今年最好的方法采用各种不计成本的模型融合，能达到66%+。而在没有深度学习之前，最好的DPM算法(也是Ross Girshick大神提出的)的平均精度是20%多。深度神经网络在这个领域确实是刷爆了。至于为什么这个平均精度看上去这么难看，那是因为它不仅评估what，还评估where。目标框和标准框之间有一点点偏移也是误差。所以看数据一塌糊涂，但实际应用的效果不至于这么磕碜。


## 语义描述算法
聊完了分类和检测，再谈谈处于刚才的分层次的顶端的语义描述。因为它的难度最高，自然地跟前两个方向相比，成熟度也较低。但相对的，应用也更有趣些。让电脑能够“看图说话”，这听着就很迷人。尤其是这里的“说话”，不但可以是“小朋友在蓝天下放风筝”，还可以是“忙趁东风放纸鸢”，甚至可以是宋词，现代诗，甚至是音乐；视频可以自动配插曲，甚至可以是用一幅图画解释另一幅图画。能衍生出比识别和检测更加丰富的智能和应用前景。
这个问题跟前面的识别和检测有什么不同呢？不同在于输出。怎么说呢？
分类问题的输出是一个类别，也就是一个整数。三分类就是[0,1,2]中的一个值，1000分类就是[0,1000)中的一个整数。检测问题呢，它输出的是一个类别，加一个位置。通常是一个整数，加上代表目标位置的矩形框，一共5个参数。每个问题的输出都是确定性很好归置的答案。
但是我们再看语义描述的输出，你就傻眼了。这里的输出是一句话啊。它不是一个整数，也不是确定数目的参数，而是可长可短的一个句子。你刚刚通过类比稍微建立起来的一点点关于神经网络的概念一定受到了挑战。
我一句话说，这个问题的通用解决方案是CNN + RNN 网络。什么是RNN网络？它是一个带有自循环结构的长度可变的网络，专门用于处理序列数据。没错，就是句子，语言等序列数据。所以这个问题的一般解法是，用CNN提取特征，然后用RNN将图像特征编码成句子。好了，问题解决了。
这个说法太简略。我还是稍微解释一下。首先解释一下RNN。

### RNN
如上图所示，RNN的结构是中间这个隐含层会向自身传播信息。从下图按时间顺序展开的计算过程看的更清晰，这个环的存在能够把前一时刻的某些信息传递到下一个时刻的计算上。而它的前向计算和反向传播跟经典的神经网络是一样的，只是这里是沿着时间轴展来算的。时间关系我就不展开了。
你现在不理解这里的W传递了什么信息并不重要。重要的是这种神经网络的结构它提供了一种顺序传播信息的机制。只要有这样的机制，就能够设计针对性的算法，传递所需要的序列信息。而回头看之前的CNN的网络结构，它恰恰没有这样的机制。它没法表达“上一时刻”和“下一时刻”这样的概念。因此，RNN能够学习序列特征。它在语音，NLP等方面的地位就跟CNN在视觉领域是同等的。这里插一句，深度学习界有“三巨头”的说法，三巨头分别是Hinton，Yann LeCun和Yoshua Bengio。前两位大家前面都听过介绍了，而Bengio正是一直研究RNN的。他们三个去年写了本“Deep Learning”，现在深度学习界的热门读物。我在推荐书目里列了。有兴趣的可以看看。这本书的中文译本只有开源的版本，并没有完成校对。而原版的书价要800大洋。所以我推荐PDF打印版。

### LRCN算法
一句话介绍了RNN之后，再用一句话介绍一种具有代表性的图形语义描述算法 - LRCN。它用CNN提取特征，然后接一簇双层RNN。图中的LSTM是RNN网络的一种。训练时这个一簇RNN的长度是可变的，对应的图像标签是句子。句子有N个单词对应的RNN stack的长度就是N+1. 最后一个<EOS>是终止符。这就完成了一个CNN + RNN的网络搭建。
大家一定还记得上面所说的深度网络学习的关键在于要有个损失函数。这里的损失函数，用的是基于前一个时刻的输出和当前时刻的输入的标签的输出的条件概率决定的。优化的不是个简单的概率分布，而是一个条件概率。

要说这个领域，我就不提在没有深度之前的方法了。之前就根本没什么成型的方法，也没有数据可以做这个事情。


## Ataraxia做什么
到这里，基本上视觉领域主要的识别方向的算法都介绍了一遍。希望大家还听的过瘾。还想继续深入某个方向的话，我们可以线下交流，或者去啃那个方向的论文。这个领域的成果是这样的，看书的话大多是些基本的算法原理，或者实战编程什么的。而具体的应用领域的算法，大体上只能看会议的文章和Survey。最后，我想谈一谈Ataraxia这个团队怎么做这个事情。我们的思考，定位是怎样的。
按惯例要来一个SWOT分析。可以看到，我们的优劣势都非常明确。
优势是我们有计算资源，有数据，有客户资源。
危险是竞争对手非常强大。
劣势是跟主流竞争对手相比，我们团队能力有差距。
机会是，我们起步的虽然晚，但现在还算是蓝海。有些方向甚至产品定义需求定义也尚不明确，我们只要努力，还是能够赶上。但是我们需要大家对整个事情有更好的认识，我们需要算法和工程的团队能很好的配合和理解，我们更需要各位同事和公司管理层的支持。

“做最懂富媒体数据的AI”这个标题是我之前做PPT时候加的。但是Google的新的Vision API和Video API出来了之后我已经不好意思这么提了。毕竟，这个世界上最残忍的就是，人家比你出身好，比你聪明，比你人多，还比你更勤奋。但是梦想还是要有的嘛，毕竟你不知道大公司做事情是不是全心全意的，就像nVidia能打败ATI一样。而我们只有这一个目标，不论结果如何，这是我们的决心所在。所以我加了个“try to”，我们还是要朝着这个方向努力。

前面说了我们客户的需求主要集中在视觉识别方向。视觉识别方向的三个层次的算法，包括其它的跟踪、分割、图像检索等外延都是客户的需求内容。这些方法不是零散的点，而是有强关联的一条条主线。最核心的内容，就是对图像的理解，以及，对视频的理解。

### Argus 和 Video Argus
所以，我们也像Google一样，整合两个主方向的算法，以Vision API和Video API作为我们的品牌产品。
Vision API，就是我们的Argus。Argus是希腊神话中年的百眼巨人。它有上百只眼睛，睡觉时也有的眼睛是睁着的。这特别符合我们的Vision API的特征：多个维度的信息提取，从不睡觉。
Argus包括以下几项：
* 内容审核：现在有鉴黄，暴恐的识别。内容审核现在属于我们的敲门砖产品，我们还是会好好打磨这个服务。后面还计划添加政治人物识别，政治事件识别等内容。
* 人脸：包括人脸的检测、特征点检测、人脸相似度，人脸聚类，性别年龄，情绪表情等等。其实盯着人脸的玩家很多，不过我们是将它定位为Argus的一部分，而不是一个单独的产品。我们的定位不是在算法上打败其它人脸算法供应商，而是说我把人脸信息作为多维标签中的一个维度，给客户更好的体验。这样哪怕客户想用别的供应商，但是没关系，别的供应商照样得乖乖的按照我们的标准来做事，不然就没法跟整个系统对接。这其实也是一个强壁垒。
* 一般目标检测：这是我们的一个纵向项目，会不断改进。我们现在有个200类的物体识别模型，后面将引入分类学的知识来做大做强。
* 场景识别：场景识别也是一个图像分类问题，就是看到一个图片告诉你是在海边，还是卧室。这个已经做的非常不错了。
* 图像描述：现在的描述就是一句话。以后可以扩展出更多花样。这个也会跟着市场走。我们也希望我们提供了这样的能力，客户能在此基础上集成自己的花样。
另一个品牌化的产品是Video Argus。它主要提供了镜头分割，视频分类，视频描述，目标检测跟踪这四方面的能力。它包含的内容比目前的Google Video API更加丰富。

可以看到，Argus和Video Argus里投提供的服务都是通用化的普适的服务。这一方面是我们在做品牌化的努力，一方面也是对图像和视频方面的通用识别能力的提升，更好地利用七牛云存储上的图像和视频，也更好地挖掘其中的价值；另外在普适的识别上面的进步，能够更好地做定制化的产品；还有一方面就是我们也在试图去定义这个尚未成熟的市场。里面有些道理是跟存储一样的，我们如果定义了客户的使用姿势，他们的迁移成本是很高的。所以我们以内容审核作为敲门砖接入了客户的数据之后，我们希望源源不断地拿出能落地的技术，然后去train我们的客户，更好地玩富媒体。比如去年我们一直做的是分类方面的生意。今年我反复提醒土土说，去找客户的时候可以问问检测方面。他一开始还觉得客户好像没这方面的需求，之前基本没有客户提起过。但是最近几趟客户拜访下来，听说我们能做这个，都纷纷表示很饥渴。
有些客户可能在大数据上比我们更专业，之前就是缺视觉这一环节的技术而已。那么这对我们也是一个学习的过程。也是一个能不断深化合作的过程。甚至通过我们的一些智能化的方案，还能拉动公司的传统产品的销售，比如存储，CDN。我觉得这是一个很好的事情。我们起家的客户都是存储等给我们积累的，而现在我们也开始有能力反哺。这对团队和对公司都是很有意义的。

### 工程化
我们对去年的项目进展是不满意的。我们也进行了很大的反思。原因有很多，商务的，技术的，招人的原因都有。其中重要的一点是我们没有做好算法的工程化，标准化。那结果就是我一直在做基本的管道建设，没有时间去做新方向的拓展。所以我也是今年才敢提看看检测方向的需求。早几个月的话，根本没工夫做。在算法人员严重不足的情况下，把算法进行标准化工程化来提升效率，这是我们的唯一出路。
上面提到的Argus和Video Argus是纵向的能力积累，而今年我们也有很多定制化的项目。比如识别一本书，检测一个家具，这类需求都不是Argus这种普适性的，但也都需要人去跟进项目。工程化就是为了使得算法工程师在具体的定制化项目上可以少花精力，尽量按照标准流程来走。所以这是我们今年的一个重点项目，关系到我们的存亡。也需要工程团队的大力支持。

### 检测算法
最后一块我想谈谈检测算法。算法方向上，我今年是把检测算法作为我们的推进的重点方向。前面也说了，识别算法已经趋于成熟，而检测算法还有比较大的提升空间。前面也说了今年有很多客户对检测很感兴趣。我觉得这是因为检测对于图像信息的挖掘更加充分，能提供更好的互动接口。比如在直播的时候可以选择直接在人脸的周围加上文字贴纸什么的。或者也有客户要求在视频里的包包的周围加个品牌，在瓶子边上加个特效。这些都需要检测。随着AR的兴起和直播变现的要求越来越深入，我相信这两年这方面的需求会越来越多。所以我们希望能花大力气在通用检测算法的提升上。通过通用检测的提升，再来带动定制化的检测项目的能力提升。
我们会用这几个方案来做检测算法的增强。
* 工程化 - 迭代，引入分类学概念，层次模型
* 多模型融合
* 无监督和半监督的方式，去充分挖掘云存储上海量数据的价值。
  * 半监督打标
  * 跟踪放大数据
  * YOLO 9000 方法
  * GAN 样本放大
这些方法都是我们试图开启云存储上的富媒体金矿的努力。那么我们也希望通过这些努力，能不断发挥出海量数据的真正价值，能把这些价值真正的落地。


## 结语

以上就是我今天的分享。扯的比较久，非常感谢大家能抽出时间来听我吹牛。我还是希望能给大家带来一点点不一样的内容。我非常希望大家听完了说，原来人工智能不像媒体上吹嘘的那么神秘，没那么神奇；当然也不是说特别特别简单特别特别水，分分钟我就能做出一个来。而是说，哦原来这玩意跟我之前听到看到的不一样，居然还有这个角度。或者说，下次有人满嘴跑火车跟你吹牛，你可以当众打脸，说别乱吹，根本没这么牛逼。如果有人听完了说这个领域非常有意思，我要花更多的时间精力去好好了解一下这个领域。那将是我觉得最高兴看到的。
最后，我还想说的一句话是，AI需要想象力，需要创造力。希望有更多人加入我们的事业，有更多的人来支持我们。我们一起来做更好玩，更有意思的AI。我们一起把七牛的人工智能做好，一起做最懂富媒体数据的AI。谢谢大家！
