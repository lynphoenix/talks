# 提纲 - 人工智能在物流行业的前景

* 自我介绍 内容介绍 3min
* 我眼中的人工智能 30min
  * 为什么是人工智能
  * 人工智能的层次
  * 人工智能的方向和应用领域
  * 人工智能的发展
  * 今天的人工智能的特点和现状

* 我能想象到的未来物流 10min
  * 格局：无人化，仓储型，信息化，
  * 无人仓储
  * 无人配送
  * 大数据管理

* 我能看到的智能物流 10min
  * 场内
    * 大数据
    * 无人仓储
    * 智能眼镜
  * 场外
    * 高速上的无人驾驶
    * 无人机配送
  * 智能物流的好处

* 智能物流的技术路径 20min
  * 机器人
    * 多机器人协作
    * 机器状态信息采集
    * 机器人的本质是感知算法和的融合
    * 无人机、无人车、机器人的安全算法
    * 电池

  * 感知算法：RFID，条码，视觉，雷达，GIS，GPS
    * 多传感器融合
    * 场内物流：
      * 物品的识别，定位    
    * 场外：
      * 运动分析
      * 路径规划

  * 交互技术
    * 交互升级

  * 大数据
    * 本质上是一个工程问题，做不好是你傻比
    * 信息采集网络
      * 使用 unified data warehouse（统一化的数据中心），让数据科学家可以安心的玩数据
    * 分析
      * 营销指导
      * 生产指导
      * 备货指导
      * 物流园选址指导
      * 设备维护

## 开场白

大家好。今天很荣幸，收到stuQ平台的邀请，来跟各位顺丰人聊聊人工智能相关的话题。首先自我介绍一下，我叫林亦宁，来自七牛云，是七牛人工智能实验室的联合创始人，目前主要负责智能的算法产品和算法平台的架构。
我们七牛是以富媒体数据存储起家的云计算服务提供商，所以我们人工智能实验室目前主要的研究和产品集中在富媒体数据的感知和数据服务方向上。
至于我个人呢，我是学控制出身的。主要的研究领域是在计算机视觉方向上，也做过注塑机、机器人、无人机等方向的项目。
今天这个话题呢，其实开始我挺伤脑筋。顺丰是一家我非常尊敬的公司，我觉得贵司是我们国家新一代企业形象的最佳名片之一吧。这是我单纯从用户体验的角度出发就能感受到的，就从快递业说，就是一股清流把。在我的认知里，顺丰对管理和对科技的追求，都是走在前列的。所以我也犯愁啊，还有什么角度是我知道人家不知道的呢？

想来想去呢，我觉得有一点不同的是，我与在座各位对人工智能这个事情的理解可能不一样，另一方面呢，我对物流跟在座的各位的理解也不一样。
那我今天呢，首先围绕人工智能谈几个点；然后呢，我的研究领域和工程经验的领域主要是在计算机视觉方向，那么我就从视觉这个方面来切入，简单介绍一下深度学习这个今天人工智能界的代表算法，的最基本的原理把。
那最后，我再以一个局外人的角度，聊聊我眼中的物流，我眼中人工智能和物流的结合点。
我也希望能给大家带来一点点不一样的角度吧。那我们就开始吧。


## 我眼中的人工智能
首先来谈第一个问题，我们为什么要研究人工智能。

### 为什么是人工智能
我认为研究人工智能是我们人类骨子里的一种渴望。
我觉得我们人类的存在是一个莫名其妙的事情。我们与这个自然环境是那么格格不入，你想象一下有人类存在之前的世界，茂密的树林，广袤的草原，汹涌的大河，湛蓝的天空。再看一眼今天的世界。我总觉得人类的出现像是生物界进化的一个意外，一个bug；而不是进化的终点。所以我们常常会觉得很孤独，从诞生第一天起，我们就渴望去了解这个世界。
我们探索世界的方向有很多，我们仰望星空，企图知道遥远的星球上是不是有和我们一样的存在，这个世界的边界在哪里，边界的外面又是什么。
我们想了解时间的边界在哪里，宇宙大爆炸之前是什么，这个世界有没有终结的一天？或者时间的存在形式跟我们以为的，我们感觉到的是不是一样。
我们也想知道是什么组成了我们今天的世界，各种物质，它们的属性是怎样的，生命是怎样诞生的，人类是怎样诞生的。
在这一切当中，也许我们首先最想要了解的是我们自己：我们每个器官是如何构成我们这样的一个系统，怎么运行，为什么会有生老病死，有没有办法达到不朽。我们的大脑又是怎样的？我们的意识是怎么来的？智能又是个什么东西？
人工智能就是尝试去了解我们的大脑，更确切的说，是要去了解智能是什么的一门学科。而人工智能的研究路径可以可以总结为Richard Feynman的这句话：What I cannot create, I do not understand. 我们通过创造一个事物去了解它。
所以人工智能的研究，就是希望通过设计一个真正能在人类所处环境中表现出像人类一样智能行为的计算机，来理解我们的大脑，理解智能。

### 什么是人工智能
所以，以上是为什么我们会研究人工智能。那么然后，我想谈的第二个问题是关于人工智能的定义。
我们一直说人工智能是一个发展中的学科。那其实潜台词就是，我们没有非常精确的对人工智能的描述。那我今天呢，仅提供一些人工智能领域的大神们们的思考角度。这个问题每个人都会有自己的思考，所以聊这个问题也就是给大家一点点新的思路。

* 图灵测试

首先看被称为“人工智能之父”的图灵怎么看这个问题。他获得这个称号的很大的原因在于写了一篇论文《计算机器与智能》。在开篇的“Can Machine Think?”里，他提出了著名的图灵测试：如果在一个回答游戏中，机器能够骗过问问题的人，使得对方觉得回答问题的是人而不是机器，那么就认定这个机器是有智能的。
在图灵看来，人类的智能最伟大的能力在于对抽象符号的处理 -- 恰好计算机也能做到。所谓听和说的过程，就是我们用恰当的语法规则对被称为“词”的概念符号的处理的过程。那看东西呢？我们是用概念符号代表物体以及物体的位置、名称和其它属性(这点在后面谈视觉的时候也会提到类似的意思，相当于高层次的视觉特征)。再比如AlphaGo下棋呢？处理的是代表不同旗子的属性和位置的概念符号。
而将智能的能力简化为抽象符号的处理(也就是计算)之后，就可以套入图灵机的理论了：人工智能，无非也就是一台图灵完备的机器，对吧。

* 神经元的数学模型

神经科学的发展似乎为图灵的假说提供了支持。1943年，Warren McCulloch和Walter Pitts提出了神经元的数学模型。该模型认为，大脑的神经元的工作方式跟数字电路中的逻辑门是一样的。如下面图中看到的，就是一个多输入线性加权，然后通过一个激活函数输出0或1。这个基本单元跟逻辑门是一样的。这个模型现在已经被广泛接受，至少好像没有人针对这个问题提出过有说服力的质疑。
而我们知道，现在计算机的所有芯片都是这样的逻辑门构成的，不论是CPU还是内存。所以在研究人员来看，人脑和计算机的基本工作原理是一致的。这也使得有很多人觉得没有必要研究大脑，而应该把精力放在设计更好的算法和理论的研究证明上。人脑有太多软弱自私等没有意义的主观情感，研究这么一个混沌的系统，对人工智能毫无意义。计算机可以做到人脑所能做到的一切，甚至更多，完全不需要研究大脑这一生理混乱无法理解的结构。

* 中文屋实验

“中文屋”实验室对这一流派的一次重要打脸。这个实验是UC Berkley的著名哲学教授John Searle专门设计来打脸人工智能的。
这个实验是这样的：让一个只懂英语的人坐在一个房间里，手头有纸笔还有一本指令手册，介绍如何处理汉字。这里的处理只涉及复制、删除、插入、重排等语法指令，而并不描述任何汉字的含义。
外面的人向里面递进中文写成的问题，里头的人按手册的说明，将中文符号组合到一起写到纸上，并最后按指令将纸条递出去。
最后别人再问外面的人：这个人会不会中文。外面的人说“必须的！”
而实际上，屋里的人(相当于CPU)，不懂中文；指令手册(软件)，只是一堆正则而已，也不懂中文。实际上，这个指令手册可以认为是人试图通过编程让计算机模仿人类的行为而积累智能的结果。John Searle的意思的，无论怎么设计这个程序，计算机都不可能具有理解力，也就不可能具有智能。他说“我也不知道智能是什么，但不论智能是什么，计算机都不具备”。

以上是关于什么是人工智能这个命题的比较重要的几个思考。可以看到，观点完全无法统一。发展中的学科就是这样。就跟什么南海问题钓鱼岛问题一样。吵吵吵到最后，结论只能是“搁置争议，共同开发”。或许有一天，我们能模拟人脑的一切输入和输出了(或者如我最近常想，如果能为大脑植入一个外挂系统，能够实现到今天为止的所有知识的快速检索和理解，有一个计算机与人脑的接口而不是用电脑去思考)，这些层面之间的相互联系可能会一目了然。但是到今天为止，这个问题还有有很大的哲学属性。多思辨有好处，但囿于争论就没有意义。我们从业人员，应该做一些更加实际的工作。


### 人工智能的三个层次
比如说呢，给人工智能问题分分层次，看看它在解决哪些层面的问题。
那第一个层次，是“感知”。其实在我看来感和知也可以分为两个小层。所谓“感”，就是将外部输入转换为大脑能够接受的信息；如果用计算机的语言来说，相当于传感器。视觉的传感器就是摄像头，听觉的传感器就是麦克风，触觉的传感器可以是压力感应器，等等。计算机能接受的传感器信号是整数或浮点数，不论是摄像头还是麦克风，最终是要通过模数信号转换形成一串无聊的数字才能被计算机所接受；而大脑也是一样，不论是视觉信号还是听觉触觉，都需要转换成生物电信号进入神经元，才能被感知。而所谓“知”，就已经是很高级的能力了。看到一个脸知道它是脸，看山是山看水是水，这个叫做“知”；听到一串声音，能辨认出它是“梦幻曲”，还是有人在喊你“吃饭”，这个是“知”；摸到一个圆圆的东西，知道它是鸡蛋，这也是“知”。但是你一定已经发现，这个“知”其实已经包含了基本的鉴别、判断、语音到文字的转换等过程。
感知层之上是“决策”。决策是在已知信息的基础上进行更高层次的判断。比如你走在街上，看到有个人向你跑来，你判断他的轨迹并不会跟你撞上；再判断他体型瘦小，速度不快，哪怕要撞上你也有足够的能力自保；此人手捧鲜花，眼神放光，你判断他是沉浸在欢愉的爱情之中，那么断不会对你造成生理上的威胁。在不到一秒钟的时间内，你已经根据“感知”层得到的各种信息，做出了三次以上的判断，并且这个判断，决定了你的行为，是避让？还是迎上？或者忽略这个人。
第三个层次是“反馈”。为什么反馈不是前两个层次中的一部分？反馈有两个重要的作用，一方面它是构成一个有执行能力的人工智能系统的重要独立环节。对于机器人系统来说，你可以认为反馈就是执行机构的监控系统。当无人驾驶的汽车执行刹车指令的时候，它需要反馈系统来传达刹车是否成功。另一方面，反馈也是学习的重要手段：通过学习，达到人工智能，或者说达到某种程度上所说的智能，靠的就是反馈，所以这是它凌驾于感知和决策之上的原因。我们大脑学习所有知识都是通过“预测 - 反馈”的路径进行的。你走进房间，看到我这个人。你脑子里会先浮现出我这个人的样子，然后再与你看到的验证。眼睛是眼睛，鼻子是鼻子，哦所以这是一个人。如果长眼睛的位置长了两个鼻子，那你一定会惊醒，然后你脑子里一些原本并不活跃的神经元开始变得活跃起来，开始学习“这个人的眼睛上居然长的是鼻子”。我们脑子进行“预测 - 反馈”的频率比你意识到的要多的多,这个过程无时不在进行。实际上我们的大脑是通过感受到的事物与预测的模式之间的匹配来学习的，这个概念可能跟传统的概念是不一样的。这个匹配的过程，靠的就是反馈。靠预测去泛化，用反馈来收敛。这是学习的原理。


### 人工智能的方向和应用领域
聊过了人工智能的三个层次后，我们可以看一眼这个领域的研究方向和应用。
左边罗列的是最近两年的研究论文和专利申请较多的研究方向。主要是语音，自然语言处理(NLP)，视觉，机器人，浅层机器学习和深度神经网络。
需要指出的是，语音和自然语言处理是两个不同层次的事情。前者是能将声音转换为文字，而后者则是要理解文字的意思。前者现在已经可以说比较成熟了，而后者还表现的像个智障。
另外，浅层的机器学习和深度神经网络指的都是算法方面的演进。而深层和浅层，实际上指的是学习算法的层数，而不是算法本身的能力深浅。其实感觉上这是深度学习业界故意挤兑人，打嘴炮的说法。
右边罗列的是人工智能的应用领域。

* 人工智能的应用领域
  * 个人助理（智能手机上的语音助理、语音输入、家庭管家和陪护机器人） 产品举例：微软小冰、百度度秘、科大讯飞等、Amazon Echo、Google Home等
  * 安防（智能监控、安保机器人） 产品举例：商汤科技、格灵深瞳、神州云海
  * 自驾领域（智能汽车、公共交通、快递用车、工业应用） 产品举例：Google、Uber、特斯拉、亚马逊、奔驰、京东等
  * 医疗健康（医疗健康的监测诊断、智能医疗设备） 产品举例： Enlitic、Intuitive Sirgical、碳云智能、Promontory等
  * 电商零售（仓储物流、智能导购和客服） 产品举例：阿里、京东、亚马逊
  * 金融（智能投顾、智能客服、安防监控、金融监管） 产品举例：蚂蚁金服、交通银行、大华股份、kensho
  * 教育（智能评测、个性化辅导、儿童陪伴） 产品举例：学吧课堂、科大讯飞、云知声

要我说这些领域的经济价值都很大，有很多可以做而还没做好的机会。至于这些机会是否属于人工智能，我看未必。总的来说，自动驾驶、医疗、安防、金融、电商这些领域，我觉得有比较清晰的未来，能很清晰地看到人工智能的进步带来的质变。而个人助理、教育，我认为目前的机会还是在于资源和大数据的整合，本质上还是由产品经理决定的；从我的粗浅的眼光，还看不到目前的算法能力能够给这些领域带来很大的机会。如果哪天NLP进一步突破了，或许情况会有不同吧。但这需要很多顶尖算法团队的努力，而不是各小算法应用团队的努力。


## 二 为什么是深度神经网络
大家知道，深度学习算法的学名叫做“神经网络”。所谓深度，是一种形象的说法，因为这种神经网络的层数非常多，或者说非常深，所以叫深度学习，或者深度神经网络。他们都是一回事。
而对这个问题再有一点深入了解的人也知道，神经网络呢，它是上世纪80年代火爆过一阵子的机器学习算法，后来因为难以实用被现在所谓的浅层机器学习的方法所取代了。
那么问题就是，为什么今天它又卷土重来了呢？

我相信在座各位也许都听说过这个问题的答案，数据爆炸和硬件升级。大家都听说过，今天大行其道的深度学习，其实上海量数据堆积出来的产品，是GPU硬件牛逼了的产品。
但你仔细想想，这里面还是有问题。为什么数据爆炸了要用今天的深度学习呢？在有深度学习之前你们做人工智能的人用的是什么方法呢？对吧。所以让我说的话，这个问题其实还有第三个要素。就是机器学习方法的革新。
上图是传统的浅层机器学习的典型流程。我在校做研究的时候就是这种方法。
样本 -> 设计特征 -> 判别算法筛选组合特征 -> 结果
本质上，特征是一种数据的映射。如果往高维数据上映射，那是为了稀疏化；如果往低维数据上映射，那是为了简化问题。然后判别算法从提取特征中提取特征或者组合特征，来得到最后的算法模型。这个过程有什么问题呢？问题出在“设计”上。设计的意思就是自己想出一个特征，俗称拍脑袋。拍脑袋是什么意思？意思是没有上帝视角，没有高屋建瓴的理论指导，说的好听点叫工程直觉，说的不好听点就是靠运气。科学史又称拍脑袋史。牛顿被苹果拍了下脑袋，拍出了万有引力。门捷列夫梦到了蛇，设计出了元素周期表。如果你设计了视觉算法中的SIFT或者HOG这样成功的特征，那么恭喜你进入了名人堂。但要说这个SIFT背后是什么原理？我会告诉你在其基础上诞生的所有理论都只是为了解释而解释。直白点的，像最简单的Haar特征一样，科学家看看猫的眼睛(因为他们没法解剖活人的眼睛啊，也有可能解剖过了，但是不敢说...)觉得是这么工作的，于是计算机视觉的研究者搬过来用，得到一个很通用的特征。
那这些简单的特征如何能够得到判别模型呢？靠的是判别算法的学习。判别算法学的是特征的排列组合。研究学者通过一堆的数学公式证明告诉你，这些特征以某种方式去组合，能够得到一个稳健的算法模型。

下图呢，是深度神经网络的流程。
样本 -> 神经网络 -> 结果
这区别就在于设计特征，判别算法的筛选组合被神经网络给取代了。同样是拍脑袋，那么我们自己不拍了，改让神经网络拍去。而且你会发现，之前设计特征和判别算法优化是两个相互独立的过程。特征提取不考虑算法的特性，算法迭代的时候也不会对特征提取过程给出指导性意见。而神经网络的工作方式则完全不同，特征提取蕴含在网络的每层参数之中，它拍脑袋的过程是由学习的目标函数直接指导的。或者说它的特征就不是拍脑袋拍出来的，而是在模型训练的过程中学出来的。这就是神经网络最大的特点，它是整体的端到端的学习方式。之前的问题是神经网络不好收敛不好调参，但到了06年的时候，Hinton提出的预训练加微调和逐层训练的学习方式，解决了这个问题。因此，它才能开始流行起来。

我最近看过一本书，书名是“人工智能的未来”。这本书对人脑的结构有一个描述：大脑皮层在外观和结构上惊人地相似，不论是主管视觉输入和主管触觉的大脑皮层区域、控制肌肉的区域、布洛卡语言区以及其他各个区域，实际上是完全一样的。大脑皮层的功能区域都遵循一个共同的算法，视觉、听觉、甚至运动输出之间没有任何差异。
神经科学家做过实验，将刚出生的雪貂的大脑经过手术重新连接，它的眼睛本来可以讲信号传输到本应是听觉发育的大脑皮层区。雪貂大脑中的听觉区形成了视觉传输路径，也就是说它们看东西时用的大脑组织本应是用来听声音的。
还有科学家用这种关系，能让盲人学会“看”。威斯康辛州立大学的生物医药工程学教授保尔*巴奇*瑞塔(Paul Bachy Rita)发明了一种在人的舌头上显示视觉模式的方法，带上他研制的装置，盲人可以通过舌上的感觉学会“看”。
他让盲人在前额戴一个小型摄像头，将视觉影像传输到舌头上的压力点上，大脑很快能学会如何正确辨别这些模式。
接受实验的人试用了这个装置后，他能看到一个球在地板上向他滚来，他伸手拿起了桌上的饮料，还能玩石头剪刀布；然后他沿着走廊往前走，看到了门，并观察了门框和门上的标牌。
这几个实验表达了什么意思呢？意思就是我们的大脑不论解决视觉、听觉、触觉，用的都是同一套机制，同一套算法。我们前面提到过，人工智能的第一个层次是感知层，也就是说人脑的感知层，对不同形式的输入，处理时用的其实是同一套算法。
是怎样的一套算法呢？看这幅图，我们的大脑解决问题用的是逐层抽象的方法。我们的大脑皮层有多个阶梯级联的，层次级的神经中枢，参见下图的V1到V5。低级区域的输出作为高级区域的输入。低级的皮层识别局部特征，越往高层，抽象程度逐渐提高。
而右边的深度神经网络，它的结构跟大脑有非常大的相似性，它也是一个逐层抽象的方式。右边的图是将各层的feature maps响应输出打印出来的结果。可以看到，最底层的神经元提取的是一些边角信息，而中间层得到的就是这些边边角角组合起来得到的眼睛鼻子等特征；到更高层的输出的就差不多是个人脸了。早上有专家也在问，卷积神经网络每层的特征怎么看，这就是可视化的方法，卷积后的feature maps可以直接可视化。

所以我们今天的人工智能，它比以往任何阶段的研究都更加接近人脑的真正工作方式。当然，这也只是比以往更加接近，而不是真正跟人脑一致，我也不敢说什么时候机器算是有智能。这个领域的发展就是这样的，年年岁岁花相似，岁岁年年人不同。
但是更加接近，而且效果非凡，仅这一点就让大家非常地兴奋了。所以我认为，这才是那些顶尖的学者、大公司都都对深度学习欢兴鼓舞，一致认为它代表了未来的重要方向的本质原因。大家在这个方向上看到了未来，至少是瞥到了一角。


## 算法介绍
以上谈了这么多，可以认为都是八卦，给大家做一个谈资吧。那第二部分，我打算简单地介绍一下深度神经网络的原理。我本人的老本行是视觉算法嘛。所以我就打算从视觉算法出发，介绍一下深度神经网络的一些基本概念。

计算机视觉，作为人工智能中感知层的一个断面，它研究的是怎么模拟人眼的功能。
人眼大致有这么几个功能，一是物体的识别，二是确定物体的形状和方位，三是判断物体的运动状态。对应到计算机视觉领域，分别是目标检测和识别；地图重建，运动分析。今天的深度学习，主要是在第一个方面努力替代传统的机器学习方法。恰好我们七牛需要的视觉算法，或者说我们的客户需要的视觉算法的业务，也都集中在视觉识别的领域。后两个领域的应用场合主要是机器人。而一旦牵涉到动作执行，往往对可靠性和准确性以及性能的要求要远大于简单的辅助判断。而神经网络由于可解释性差的弱点，因此在这两个方面的进展仍然缓慢。

识别这一领域的视觉算法从简单到复杂，大致分为这几个层次：
* 最简单的是分类算法，给你一张图片，你说其中有人或者没有，色情或者不色情，这叫分类。我们七牛做的色情暴力的鉴别就是这一层次的问题。深度学习算法最先突破的ImageNet竞赛，也是分类问题。
* 比分类算法更进一步的是检测。检测问题不仅回答是不是，有没有，还要回答在哪里。比如人脸检测就是典型的问题。
* 第三个层次是语义描述。通俗地说，就是看图说话或者看视频说话。比如图中的这个例子。
这三个层次一个比一个更复杂。也就是说，一个比一个含金量高。虽然我没法很定量地说，一个检测的API应该比一个分类的API贵多少钱，而且也不是说做检测问题比做分类问题活的更好更有商业价值。但是从趋势上看，大家会越来越关注高层次的应用；而低层次的应用，由于门槛低，会越来越趋向于成为标准品。

当然，这只是个简单维度的层次划分，还有些很重要也很有意思的方向，也是我们关注的重点。比如跟踪和检测常常是打包在一起的；图像的语义分割也是很重要的方向，在视频直播等场景下有很大的商业价值，对无人驾驶来说更是至关重要。分类问题也不只有简单的“是/不是”这一个维度，比如海量商品的检索，细粒度特征的检索都是分类问题中的高精尖方向。另外，像超分辨率、风格画这些有意思的图片处理算法，也是计算机视觉领域常常受到关注的算法。

我今天就介绍一下识别这条主线上的分类、检测、描述三种算法。

## 分类算法

### LeNet
先看最简单的分类算法。这是LeNet网络，Hinton的学生Yann LeCun在20世纪90年代为美国各金融机构开发的手写字符识别系统用的网络结构。由于它简单，又具备了现在的深度神经网络的大部分要素，而且效果特别好。因此被大家广泛用于解释神经网络。就像图像处理界总是拿Lena那张照片解释算法一样。话说Lena的真实身份是一个模特，这张照片其实是PlayBoy杂志里的彩页，但也不知为何就在计算机视觉领域被广泛应用了，成为这一领域广大宅男和科学家最知名的女神。她的影响力之大以至于有一次top的学术会议还请她作为嘉宾，不知道是不是跟现在的工业展会请苍老师是一回事...

### 卷积层
扯远了，接着说这个LeNet。它由几个要素组成。首先是Convolution，中文名叫卷积。在神经网络的术语中这个操作就称为“卷积层”。可以看到这个网络中有两处Convolution，代表它有两个卷积层。卷积这个名词听着不好懂，我换个名字相信有点数学知识的人都能明白，叫做“加权求和”。没错，卷积就是一个图像块每个点有不同的权重，加权求和之后作为中心点的输出值。卷积层就是用N个这样的加权模板对图像上的所有图像块求，得到一个N个新的图像。

### pooling 层
然后注意到网络里有两处Subsampling，这个叫降采样。学术名叫pooling层。其实就是缩小图像。只是这里有些不同的方法来选择缩小后图像的每个点的像素值。假设长和宽各缩短一半，那么就是在原图的2x2图像块中选择一个点：如果是4个点的像素平均一下，就叫average-pooling，如果是取4个点的最大值，就叫max-pooling。

### CNN
卷积层和pooling层交替的这么个结构，就是卷积神经网络的典型结构。它的英文名叫CNN。是不是好像在哪里听过。但凡听过一星半点的计算机视觉的介绍，比如人脸识别，应该都听到过这个名字。现在只要是跟视觉识别沾边的领域，全都是用的这种流派的算法。区别仅在于细节不同，损失函数不同。为什么？据刽子手(屠杀过很多青蛙、猴子的科学家们)说，我们的眼睛的工作方式就是这样的。就是上面说的，逐层抽象。所以你看，人工智能有时候是很没道理的，大家都是拍脑袋，无非看谁拍出来的效果好而已。到后来大家聪明了，都去找搞生物的去拜师了。

### 全连接层
第二个pooling后面有两个full connection，就是全连接层。顾名思义，把最后一个pooling输出的每个点和本层输出的每个点之间都连接上就叫全连接。原本卷积 + Pooling这个操作得到的是一个二维图片，全连接就是把最后的二维图片拉直，变成了一维的数组。只有一个全连接层还嫌不够神经网络(非线性程度不够)，还搞两个全连接层。最后再用Softmax函数做个分类输出。从前面的CNN结构，到全连接，最后到Softmax的分类输出，这就是从输入的图像(手写字符)到输出的判别结果(A-Z，0-9)的整个过程。

### 优化过程
介绍完网络架构，深度神经网络的决策过程也就清楚了。当然深度神经网络的神奇不是因为这个决策过程而是因为它的学习能力。它如何学习呢？它的学习方法叫做迭代优化，它有一个确定的、非凸的优化目标(损失函数最小化)，一点一点地向正确的方向拱。所谓损失函数，顾名思义就是衡量这个决策造成了多少损失。什么是损失呢？在分类问题里，就是原本是A，预测成了B。这就叫损失函数。计算机视觉的数学定义往往非常简单。
基本上所有机器学习问题采用的都是迭代优化的方式。深度神经网络采用的优化方法就是BP算法，正是20世纪80年代Hinton提出的优化算法。具体的理论计算过程本文就不展开了。但是要提一点，为什么同样是BP算法，20世纪80年代难以优化而现在这么成功。前面提到了Hinton在2006年用了“预训练+微调”以及"逐层优化"的方式降低了神经网络训练的难度。而到了今天大家的常规做法是用通用的分类网络(即ImageNet分类问题训练的网络)训练好的模型，针对自己应用的具体问题去调优。就是“预训练+调优”的过程。而现在的网络已经足够强大，已经不需要“逐层优化”这一方法了。

OK，这就大体讲完了分类网络的结构，推理过程，以及学习的过程。当然实际上里头的门道是很多的，有很多环节都有不同的方法去调优。但是到今天为止，主流的方法都是在这个体系之内。
分类算法是深度神经网络在计算机视觉领域最早也是最成功的应用，也是其它问题的基础。有了分类算法和CNN的概念打底，我们可以接着谈其它的应用领域了。

## 检测算法
检测算法要回答两个问题：what & where。在有深度精神网络之前我们是怎么做检测算法的呢？通常我们是用暴力搜索来解决where，用上面的分类算法来解决what。我们用一个分类器在全幅图像上用滑动窗口去扫描，也就是在全图的每个位置去问一句“是不是你啊？”而由于要问的东西有大有小，所以通常还要将图像进行不同尺度的缩放，也就是缩放出一个从小到大的N幅图像组成的图像金字塔，在整个图像金字塔的每个位置拿着分类器模型去问“是不是你”。最后，再用“非极大值抑制”的方法将可能重合的检测结果合并去重。这个效率是令人发指的，一张图下来，每个位置问一遍，至少10w次，来一套金字塔，就是百万次判断...
基于CNN的图像分类算法大获成功之后，马上就有人将其用到了检测领域。
一开始是Yann LeCun用CNN模型去滑动窗口扫描，这样做准确率是提升了。但是速度实在是没法忍受。
然后一位算法和代码都已入化境的Ross Girshick大神提出了一种算法叫做RCNN - Region CNN，用显著性特征取代了滑动窗口。所谓显著性特征可以理解为图像中特别引人注意的部分，比如色彩特别鲜艳，纹理特别丰富。大神用这种方法在每张图中提取大约2000个可能出现目标物体的区域，然后在每个位置上用CNN模型去问“在不在啊”。可以看到，2000 vs 百万，优势很明显嘛。
当然这种算法不是深度学习的胜利。不过没关系，大神再接再厉。第二年他又提出了Fast RCNN，第三年又提出了Faster RCNN。考虑到Fastest这个词不好随便用，要不然以后没得写了，所以他结束了灌水。于是在Faster RCNN这个方法中将显著性特征这个提取可能目标位置的阶段也用CNN给取代了。从而检测算法的速度得到了极大的提升，基本上可以达到实时了。
再后来，大神玩了个大的，设计了个一步走的检测网络，再也不用分开“提取可能目标位置”和“你谁啊？”两个阶段了。取了个洋气的名字叫做YOLO - you look only once。它的效果并不如RCNN系列，但好处是“特！别！快！”。速度是Faster-RCNN的20倍。所以啊，老同志没有出“Fastest RCNN”，必有深意啊！
有了之前的分类问题打底，我们稍微介绍一下YOLO的网络结构。可以看到它也是卷积 - MaxPooling，卷积 - MaxPooling...然后全连接。最后输出层是一个7x7x30的矩阵。它将整个输入图像分成了7x7的网格，这个7x7x30的矩阵就代表了每个网络位置的预测。30是什么？每个网格会预测各个目标类别的分类概率C(有20个分类因此C=20)，会预测2个目标框，每个框对应x,y,w,h的坐标描述，以及目标框的置信概率s，所以每个框一共是5个参数。20 + 5 * 2 = 30。网络结构介绍完毕。
然后是什么呢？损失函数。检测问题和分类问题的不同，区别就在这个损失函数上。前面说了，分类问题的损失函数就是A判别成B了，损失！前面还说了，检测有两个问题：what和where。所以检测问题的损失函数有两项：是/不是，位置偏差。
对了RCNN系列和YOLO系检测算法在ImageNet的Detection Session上的平均精度能达到60%左右，今年最好的方法采用各种不计成本的模型融合，能达到66%+。而在没有深度学习之前，最好的DPM算法(也是Ross Girshick大神提出的)的平均精度是20%多。深度神经网络在这个领域确实是刷爆了。至于为什么这个平均精度看上去这么难看，那是因为它不仅评估what，还评估where。目标框和标准框之间有一点点偏移也是误差。所以看数据一塌糊涂，但实际应用的效果不至于这么磕碜。


## 语义描述算法
聊完了分类和检测，再谈谈处于刚才的分层次的顶端的语义描述。因为它的难度最高，自然地跟前两个方向相比，成熟度也较低。但相对的，应用也更有趣些。让电脑能够“看图说话”，这听着就很迷人。尤其是这里的“说话”，不但可以是“小朋友在蓝天下放风筝”，还可以是“忙趁东风放纸鸢”，甚至可以是宋词，现代诗，甚至是音乐；视频可以自动配插曲，甚至可以是用一幅图画解释另一幅图画。能衍生出比识别和检测更加丰富的智能和应用前景。
这个问题跟前面的识别和检测有什么不同呢？不同在于输出。怎么说呢？
分类问题的输出是一个类别，也就是一个整数。三分类就是[0,1,2]中的一个值，1000分类就是[0,1000)中的一个整数。检测问题呢，它输出的是一个类别，加一个位置。通常是一个整数，加上代表目标位置的矩形框，一共5个参数。每个问题的输出都是确定性很好归置的答案。
但是我们再看语义描述的输出，你就傻眼了。这里的输出是一句话啊。它不是一个整数，也不是确定数目的参数，而是可长可短的一个句子。你刚刚通过类比稍微建立起来的一点点关于神经网络的概念一定受到了挑战。
我一句话说，这个问题的通用解决方案是CNN + RNN 网络。什么是RNN网络？它是一个带有自循环结构的长度可变的网络，专门用于处理序列数据。没错，就是句子，语言等序列数据。所以这个问题的一般解法是，用CNN提取特征，然后用RNN将图像特征编码成句子。好了，问题解决了。
这个说法太简略。我还是稍微解释一下。首先解释一下RNN。

### RNN
如上图所示，RNN的结构是中间这个隐含层会向自身传播信息。从下图按时间顺序展开的计算过程看的更清晰，这个环的存在能够把前一时刻的某些信息传递到下一个时刻的计算上。而它的前向计算和反向传播跟经典的神经网络是一样的，只是这里是沿着时间轴展来算的。时间关系我就不展开了。
你现在不理解这里的W传递了什么信息并不重要。重要的是这种神经网络的结构它提供了一种顺序传播信息的机制。只要有这样的机制，就能够设计针对性的算法，传递所需要的序列信息。而回头看之前的CNN的网络结构，它恰恰没有这样的机制。它没法表达“上一时刻”和“下一时刻”这样的概念。因此，RNN能够学习序列特征。它在语音，NLP等方面的地位就跟CNN在视觉领域是同等的。这里插一句，深度学习界有“三巨头”的说法，三巨头分别是Hinton，Yann LeCun和Yoshua Bengio。前两位大家前面都听过介绍了，而Bengio正是一直研究RNN的。他们三个去年写了本“Deep Learning”，现在深度学习界的热门读物。我在推荐书目里列了。有兴趣的可以看看。这本书的中文译本只有开源的版本，并没有完成校对。而原版的书价要800大洋。所以我推荐PDF打印版。

### LRCN算法
一句话介绍了RNN之后，再用一句话介绍一种具有代表性的图形语义描述算法 - LRCN。它用CNN提取特征，然后接一簇双层RNN。图中的LSTM是RNN网络的一种。训练时这个一簇RNN的长度是可变的，对应的图像标签是句子。句子有N个单词对应的RNN stack的长度就是N+1. 最后一个<EOS>是终止符。这就完成了一个CNN + RNN的网络搭建。
大家一定还记得上面所说的深度网络学习的关键在于要有个损失函数。这里的损失函数，用的是基于前一个时刻的输出和当前时刻的输入的标签的输出的条件概率决定的。优化的不是个简单的概率分布，而是一个条件概率。


以上是通过简单的视觉算法原理，希望这个简单的算法原理呢，能给大家一些关于深度学习，神经网络的基本概念。
到这里呢，识别领域的三个层次的算法都基本梳理了一遍。
这些就是我今天要分享的主要内容。希望大家听的还轻松。


## 我能想象到的未来物流

我最后想分享百度刚离职的人工智能总架构师吴恩达的一个观点，实际上可能也不是他先提出来的。他说，人工智能就是我们这个时代的电能，是我们这个时代的能源革命。它注定将为各个行业赋能，这是我们所有人的机会与挑战。
比如在座各位，也是一致认同人工智能在物流行业的前景，对吧。
我以前搞过无人机，在学校的时候还搞过智能仓储、RFID等方面的项目，所以对这个行业可以说略懂一二。所以我对物流行业也有一些自己的想象把。最后我就分享一下我认为这个行业会发生的变化。我描绘的不是天马行空的未来想象，而是在我看来实实在在应该要发生的改变。当然，可能是5年，10年，但不会是50年。

在我看来未来物流的场景应该是这样的：
今天我下单，给身在异地的女朋友快递一个包裹。我不需要打电话给顺丰客服，而是直接告诉亚马逊的echo：“这个包裹，你帮我寄给我女朋友吧，对了，这个包裹大概有10斤重，挺沉的”。
为什么我提的是Echo呢？因为Echo是现在唯一具备远场识别功能的语音助手。也就是说，我不需要像跟Siri说话一样将手机拿到嘴巴边上，而是可以在有嘈杂的环境噪声下完成这一指令。
然后Echo会干两件事情，一是它会找到我女朋友的住址电话等信息。这个信息可以是之前我曾经口头告诉过它的。二是它会联系顺丰，对接到顺丰的管理系统下单。

我是顺丰的老客户，所以你们的后台系统了解我的家庭住址，根据我的住所的环境信息和我要寄的包裹的特征，选择取货方式。
如果我住的是别墅，能够飞无人机，包裹也不重，就派个无人机来取货，我只需要把它放在无人机的取货点即可。
如果包裹很重，我家离最近的配送站也不近，飞机的续航时间不够，那么可能可以派无人车来。
如果我住的是公寓，而我也很懒又进忙，不愿意自己将包裹拿到最近的服务站点，那么就只能麻烦师傅开车来取货了。
而这些逻辑的判断，在Echo下单的时候，就可以完成并开始调度。

不论是哪种取货方式，货物最终会乘坐无人驾驶的卡车进入本地的中心仓库。这是一个24小时工作的无人立体仓库。
卡车停下来之后马上会自动将货厢卸下。多个六自由度的机械手围着货厢，将货物一个个搬到分拣台。

分拣台连接着并行的多条传送带，分别通往不同的上一级集散中心，或者同级的其它目的地，或者是在缓存区存放一段时间。大数据系统和计算机视觉系统会找到每个包裹应该走上哪条传送带，并由散布在各条传送带周围的六自由度机械手将其拾取到对应的传送带上。

最终我的包裹会进入一个通往上一级集散中心的货物池。 这个货物池是一个立体货架，我的包裹现在是矩阵中的一个点。等到下一个出库时间节点时刻，由送它上架提升机或者Kiva机器人扛着它与其它一票前往下一个集散中心的包裹一起上路。

经过若干次这般的入库出库，最终它又跟着无人卡车到达了终端配送点。然后配送终端会选择一个合适的方式投递我的包裹。

这是我，一个普通用户想象的未来快递场景。这里最鲜明的特征，一是无人化，二是信息化。
今年看到京东提出了“三无”，无人机、无人车、无人仓。我觉得这是眼见能落地的未来。作为算法研究人员，我们也会担心里面的人工智能算法是否靠谱。但是实际上我们考虑这个问题的时候，出发点不应该是“算法出错了怎么办？”，而是“出错概率是否比人工低？”
当我们这样考虑问题时，会发现很多方案都是可行的，或者是即将可行。

另一个方面是信息化，这点我相信不必多说，在座的应该都比我专业。我们是做工具的，而各位是用工具的，各位是真正的行业应用的专家。但是我从另一个角度看问题，快递公司最终其实会成为一个人口普查办公室。所有用过顺丰快递的人，我们的家庭信息其实都会在顺丰的庞大数据库中。我们住在哪里，这个地方适合飞无人机还是适合开车或者只能走路，甚至我们寄多少东西，买什么东西，这些信息都会在顺丰的系统里。那怎么用这些数据，如何维护数据安全，都会关系到一个快递公司的成败。


以上是对个人消费者来说的体验，对于行业来说，我认为还有第三点趋势也尤为重要，就是仓储化。我认为顺丰这样的快递公司，最后会吃下另一块业务，就是仓储。电商将越来越少自己的仓储，而是将租用网点越来越密集的快递公司，各地的物流园会承担越来越多的这种角色。这种集中化的管理，能带来极大的运转效率提升，也理应给快递公司带来丰厚的利润。


## 我能看到的智能物流
那么在上述的合理性假设中，从我这个人工智能从业者看来，正在发生的有哪些呢？
我是这么看的：
首先用Echo下单，这个事情现在就可以做；而且这是个有意义的事情。
然后最后一公里的无人机和无人车或许现在不靠谱，但是高速路上的无人卡车是非常值得落地的项目。
立体仓库现在已经广泛应用了，给它们配上自动分拣系统也会是顺理成章的发展。而且无人仓库有多种形态，可以适合不同的仓库类型。
作为无人分拣系统的重要补充，智能眼镜是一个很好的工具。
物流中心和高密度网点，这个趋势是实实在在的对吧。但是我


## 结语

以上就是我今天的分享。扯了比较久的时间，非常感谢大家能抽出时间来听我吹牛。我还是希望能给大家带来一点点不一样的内容。
每一代人都有自己的使命，比如我们的祖辈的使命是革命，我们的父辈的使命是建设这个国家。而我们这代人的使命是创造新的世界，创造一个自由快乐的新世界，创造一个用智能机器彻底改造我们生活的新世界。
我觉得我们这代人很幸运，我们在经历的是一个大时代。人类历史上很少经历这样的大时代，人们的世界观、生活方式、政治秩序都在发生剧烈的动荡。今天，人工智能正在改造我们的生活的各个方面。前面20年，通讯和互联网已经将我们的生活方式颠覆了一遍，而这还只是上半场。
人工智能会接着将我们的生活重新改造一遍，很多工作岗位会被机器取代，又会有很多新的工作出现。甚至是整个世界的秩序都会被改造一遍。未来将走向何方，是需要我们每个人去思考去探索去把握的。毫无疑问的是，这是一场革命，每个人每个公司都面临选择。是拥抱革命，还是被别人革命。我们想要怎样一个未来？我们想要怎样一个世界？选择都在我们自己脚下。我希望大家一起努力，通过人工智能，去改造我们自己的行业，改造我们的物流，改造我们的生活，建设更加美好的未来。谢谢大家！


## 智能物流的技术路径

* 视觉
  * 检测
  * OCR
  * 分割
  * 运动分析
  视觉检测
  分割
  订单识别
  元件检验
  视觉定位
  识别
  定位
  运动分析
  信息融合

* 语音

* 机器人
AGV
机械手
立体仓库
货架穿梭车
拣货机器人 Kiva
分拣机器人
无人机
无人车
多机器人协作
安全算法
电池
设备状态采集


* 大数据
统一数据中心
营销指导
生产指导
库存指导
物流园选址指导

设备维护
机器状态
货物信息
订单信息
配送过程信息
人员信息
车辆信息
